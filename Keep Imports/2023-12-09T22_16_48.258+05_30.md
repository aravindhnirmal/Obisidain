[[Keep/Colour/DEFAULT]] [[Keep/Archived]] 



1. **Compute Dot-Product of Two Vectors Using Parallel Reduction**:

```cpp
[[include]] <iostream>
[[include]] <cuda_runtime.h>

[[define]] N 1024

__global__ void dotProduct(int* a, int* b, int* result) {
    int tid = threadIdx.x + blockIdx.x * blockDim.x;
    int partialSum = 0;

    if (tid < N) {
        partialSum = a[tid] * b[tid];
    }

    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        __syncthreads();
        if (tid < stride) {
            partialSum += a[tid + stride] * b[tid + stride];
        }
    }

    if (tid == 0) {
        result[blockIdx.x] = partialSum;
    }
}

int main() {
    int a[N], b[N];
    int* d_a, * d_b, * d_result;
    int result = 0;

    for (int i = 0; i < N; ++i) {
        a[i] = i;
        b[i] = i * 2;
    }

    cudaMalloc((void**)&d_a, N * sizeof(int));
    cudaMalloc((void**)&d_b, N * sizeof(int));
    cudaMalloc((void**)&d_result, N * sizeof(int));

    cudaMemcpy(d_a, a, N * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, N * sizeof(int), cudaMemcpyHostToDevice);

    int threadsPerBlock = 256;
    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;

    dotProduct<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_result);

    int* tempResult = new int[blocksPerGrid];
    cudaMemcpy(tempResult, d_result, blocksPerGrid * sizeof(int), cudaMemcpyDeviceToHost);
    for (int i = 0; i < blocksPerGrid; ++i) {
        result += tempResult[i];
    }
    delete[] tempResult;

    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_result);

    std==cout << "Dot Product: " << result << std==endl;

    return 0;
}
```

2. **Program on Matrix Transpose Using Shared Memory**:

```cpp
[[include]] <iostream>
[[include]] <cuda_runtime.h>

[[define]] N 16
[[define]] BLOCK_SIZE 4

__global__ void matrixTranspose(int* input, int* output, int width, int height) {
    __shared__ int tile[BLOCK_SIZE][BLOCK_SIZE];

    int xIndex = blockIdx.x * blockDim.x + threadIdx.x;
    int yIndex = blockIdx.y * blockDim.y + threadIdx.y;

    int indexIn = yIndex * width + xIndex;
    int indexOut = xIndex * height + yIndex;

    if (xIndex < width && yIndex < height) {
        tile[threadIdx.y][threadIdx.x] = input[indexIn];
    }

    __syncthreads();

    if (xIndex < height && yIndex < width) {
        output[indexOut] = tile[threadIdx.x][threadIdx.y];
    }
}

int main() {
    int input[N][N];
    int output[N][N];

    for (int i = 0; i < N; ++i) {
        for (int j = 0; j < N; ++j) {
            input[i][j] = i * N + j;
        }
    }

    int* d_input;
    int* d_output;

    cudaMalloc((void**)&d_input, N * N * sizeof(int));
    cudaMalloc((void**)&d_output, N * N * sizeof(int));

    cudaMemcpy(d_input, input, N * N * sizeof(int), cudaMemcpyHostToDevice);

    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE);
    dim3 blocksPerGrid(N / BLOCK_SIZE, N / BLOCK_SIZE);

    matrixTranspose<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, N, N);

    cudaMemcpy(output, d_output, N * N * sizeof(int), cudaMemcpyDeviceToHost);

    cudaFree(d_input);
    cudaFree(d_output);

    for (int i = 0; i < N; ++i) {
        for (int j = 0; j < N; ++j) {
            std::cout << output[i][j] << " ";
        }
        std==cout << std==endl;
    }

    return 0;
}
```

3. **Find 1-D Stencil Using Constant Memory**:

```cpp
[[include]] <iostream>
[[include]] <cuda_runtime.h>

[[define]] N 16

__constant__ int stencil[3] = {1, 0, -1};

__global__ void applyStencil(const int* input, int* output, int width) {
    int tid = threadIdx.x + blockIdx.x * blockDim.x;

    if (tid >= 1 && tid < width - 1) {
        int result = 0;
        for (int i = -1; i <= 1; ++i) {
            result += input[tid + i] * stencil[i + 1];
        }
        output[tid] = result;
    }
}

int main() {
    int input[N] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};
    int output[N];

    int* d_input;
    int* d_output;

    cudaMalloc((void**)&d_input, N * sizeof(int));
    cudaMalloc((void**)&d_output, N * sizeof(int));

    cudaMemcpy(d_input, input, N * sizeof(int), cudaMemcpyHostToDevice);

    int threadsPerBlock = 256;
    int blocksPerGrid= (N + threadsPerBlock - 1) / threadsPerBlock;

    applyStencil<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, N);

    cudaMemcpy(output, d_output, N * sizeof(int), cudaMemcpyDeviceToHost);

    cudaFree(d_input);
    cudaFree(d_output);

    for (int i = 0; i < N; ++i) {
        std::cout << output[i] << " ";
    }
    std==cout << std==endl;

    return 0;
}
```

These CUDA programs perform the specified tasks without comments for a more concise view.
