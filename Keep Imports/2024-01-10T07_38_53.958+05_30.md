[[Keep/Colour/DEFAULT]] 

**Overview of Hadoop:**

- Hadoop is composed of the Common, HDFS, and MapReduce components.
- HDFS is a distributed file system for storing large files on a cluster of machines.
- Hadoop MapReduce is a framework for processing data stored in HDFS.
- Frameworks like HBase, Pig, and Hive are built on top of Hadoop.

**Understanding HDFS:**

- HDFS serves the purpose of storage and is used for efficient processing with MapReduce.
- Both HDFS and MapReduce are co-deployed in a single cluster.
- HDFS enables moving computation to data, avoiding the physical separation of storage and processing.
- It's a distributed file system designed for high-throughput access to data.
- Components of HDFS include the Name Node (master) responsible for file and directory management, and Data Nodes (slaves) deployed on each machine for storage.
- HDFS is a distributed file system designed for high-throughput data access. It consists of a Name Node (master) responsible for managing files and directories, and Data Nodes (slaves) deployed on individual machines for storage. Data Nodes handle read and write requests.
- A secondary node handles checkpoints, aiding in Name Node recovery during failures. HDFS maintains data reliability by creating multiple replicas of data blocks and distributing them across the cluster. This enhances accessibility and ensures reliability.
- In the context of processing, HDFS pairs with the MapReduce framework. MapReduce's input phase involves parallel map tasks with no sharing concept. The output phase aggregates map phase results into a single output. HDFS stores both input and output of MapReduce jobs, completing the data processing cycle.
MapReduce is a fundamental programming model in the Hadoop ecosystem designed for processing and analyzing large datasets.
- **Programming Model:** Hadoop enforces the use of a specific programming model called MapReduce for cloud applications running on its platform.
- **Two Phases:** MapReduce operates in two main phases:
    
    - **Map Phase:** In this phase, data is processed and transformed in parallel across the distributed nodes. Each piece of data is broken down into key-value pairs.
    - **Reduce Phase:** The processed data from the map phase is aggregated, summarized, and the final output is generated.
- **Map and Reduce Functions:** Developers need to define two functions, known as the Map function and the Reduce function, to work within the MapReduce framework.
    
    - **Map Function:** The programmer specifies how input data is transformed into intermediate key-value pairs. This function is applied in parallel across the dataset.
    - **Reduce Function:** The Reduce function takes the output of the Map phase (intermediate key-value pairs) and performs further processing to produce the final output.
- **Input Data Format:** Input data for the MapReduce framework consists of simple key-value pairs. This data format is essential for the map and reduce functions to process the information effectively

**MapReduce Workflow:**

**Step 1: Map Phase**
- One block of data is processed by a single mapper at a time.
- Developers define their own business logic within the mapper, tailored to specific requirements.
- The Map phase runs concurrently across all nodes in the cluster, processing data blocks in parallel.

**Step 2: Intermediate Output**
- The output generated by the mapper is known as the intermediate output.
- This intermediate output is temporarily stored on the local disk of each individual node where the mapper ran.
- The intermediate output is not stored on HDFS to prevent unnecessary replication and data duplication.

**Step 3: Shuffling and Copying**
- The intermediate output from mappers is shuffled or copied to reducer nodes.
- Reducer nodes are regular slave nodes, but they execute the reduce phase.
- Shuffling involves the physical movement of data over the network.

**Step 4: Merging and Sorting**
- After all mappers have completed their tasks and their intermediate output is shuffled to reducer nodes, the intermediate outputs are merged and sorted.
- This merged and sorted data serves as the input for the subsequent reduce phase.

**Step 5: Reduce Phase**
- The reduce phase is the second part of the MapReduce process.
- During this phase, developers specify their own custom business logic according to requirements.
- The input to a reducer is sourced from all the intermediate outputs generated by the mappers.
- The output of the reduce phase is the final result, and it is written to HDFS for storage.

In essence, the MapReduce process involves mapping, intermediate output generation, shuffling, merging, sorting, reducing, and finally producing the desired result that's stored on HDFS. This parallel and distributed processing approach efficiently handles large-scale data processing tasks.












The processing flow for a MapReduce program is given below:

• The input data is split into chunks, each of which is sent to different Mapper processes.
•The output of the Mapper process includes key-value pairs.
•The result of the Mapper process is partitioned based on the key and is sorted locally.
•The Reduce function gets this sorted key-value data for one key, processes it and generates the output
key-value pairs.
•The key and the value classes should be in serialized manner by the framework and hence, need to
implement the Writable interface.





•(Map function in Word Count)
•Reduce function in Word Count










